{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkz06QdygNaIx3aHRtYOhD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **PFD Query using langchain and AstraDB**"],"metadata":{"id":"3-Lz2x3LhSb1"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"4BOgIyJHhMPg","executionInfo":{"status":"ok","timestamp":1739808353119,"user_tz":-330,"elapsed":4916,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"outputs":[],"source":["!pip install -q cassio datasets openai tiktoken"]},{"cell_type":"markdown","source":["Importing all the necessary packages\n"],"metadata":{"id":"rb0qHFkVwahW"}},{"cell_type":"code","source":["!pip install langchain-community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3F2toQkw83D","executionInfo":{"status":"ok","timestamp":1739808355082,"user_tz":-330,"elapsed":1969,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"07f9bb51-8efe-40dd-d56f-de9ced36af5a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.17)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.35)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.18 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.8)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n","Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (0.3.6)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"]}]},{"cell_type":"code","source":["!pip install google-generativeai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGtoKtQXirOZ","executionInfo":{"status":"ok","timestamp":1739808363877,"user_tz":-330,"elapsed":5084,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"272c1151-14e3-4f5e-8c18-c3a54847dfaa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.19.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"]}]},{"cell_type":"code","source":["from langchain.vectorstores.cassandra import Cassandra\n","from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n","from langchain.llms import OpenAI\n","from langchain.embeddings import HuggingFaceEmbeddings"],"metadata":{"id":"Kv_KfRewwdpF","executionInfo":{"status":"ok","timestamp":1739808366081,"user_tz":-330,"elapsed":2208,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVTC7ej7edmH","executionInfo":{"status":"ok","timestamp":1739808371253,"user_tz":-330,"elapsed":5175,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"9c3690d0-e1db-4315-9a03-5f0b304cbb40"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"]}]},{"cell_type":"code","source":["from PyPDF2 import PdfReader"],"metadata":{"id":"zT_rq5kleiai","executionInfo":{"status":"ok","timestamp":1739808371614,"user_tz":-330,"elapsed":367,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:pxYZmJNJJgaSnsPDjhCALtEI (Replace with your Token))\"\n","ASTRA_DB_ID = \"2964c6ae-6a4b (Replace with your Astra DB id))\"\n"],"metadata":{"id":"E8ASwkkOeouQ","executionInfo":{"status":"ok","timestamp":1739808372430,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as GenAI\n","GEMINI_API_KEY = \"API-KEY\""],"metadata":{"id":"86wx8WlXheWN","executionInfo":{"status":"ok","timestamp":1739808372831,"user_tz":-330,"elapsed":406,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["pdf_reader = PdfReader('/content/Question bank_TSA.pdf')"],"metadata":{"id":"a6apeyx1kMmD","executionInfo":{"status":"ok","timestamp":1739808379162,"user_tz":-330,"elapsed":598,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["raw_text = ''\n","for i, page in enumerate(pdf_reader.pages):\n","    text = page.extract_text()\n","    if text:\n","        raw_text += text\n","raw_text[555:1000]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"j40mB7jHkVop","executionInfo":{"status":"ok","timestamp":1739808386420,"user_tz":-330,"elapsed":5222,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"fc60fa09-31d9-4167-90f0-b7636a4f87ad"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'  \\nText Preprocessing and Wrangling – Text tokenization – Stemming – Lemmatization – Removing stop - \\nwords – Feature Engineering for Text representation – Bag of Words model - Bag of N -Grams model – \\nTF-IDF model – Case study: Analysis of Text Preprocessing using NLTK, Implementation of TF -IDF \\nmodels.  \\n \\nPart – A \\n1 State text tokenization in text preprocessing?  \\n\\uf0b7 Tokenization  is the process of dividing text into a set of meaningful '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Initializing connection to Data Base"],"metadata":{"id":"sfnZ43dilbrT"}},{"cell_type":"code","source":["import cassio\n","cassio.init(token = ASTRA_DB_APPLICATION_TOKEN, database_id = ASTRA_DB_ID)"],"metadata":{"id":"0B3K_EDolYhq","executionInfo":{"status":"ok","timestamp":1739808390334,"user_tz":-330,"elapsed":2977,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!pip install vertexai"],"metadata":{"id":"sQIxgS1_p1RH","outputId":"4af08456-b549-4b2b-a754-a12b60901979","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vertexai\n","  Downloading vertexai-1.71.1-py3-none-any.whl.metadata (10 kB)\n","Collecting google-cloud-aiplatform==1.71.1 (from google-cloud-aiplatform[all]==1.71.1->vertexai)\n","  Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.19.2)\n","Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.27.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.26.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (4.25.6)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (24.2)\n","Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.19.0)\n","Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (3.25.0)\n","Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.14.0)\n","Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.0.7)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.10.6)\n","Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.16)\n","\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.66.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.32.3)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.62.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (5.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (4.9)\n","Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.4.1)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.7.2)\n","Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.8.2)\n","Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.14.0)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.6.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (4.12.2)\n","Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.11/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.26.4)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (0.6.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai) (2025.1.31)\n","Downloading vertexai-1.71.1-py3-none-any.whl (7.3 kB)\n","Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: google-cloud-aiplatform, vertexai\n","  Attempting uninstall: google-cloud-aiplatform\n","    Found existing installation: google-cloud-aiplatform 1.79.0\n","    Uninstalling google-cloud-aiplatform-1.79.0:\n","      Successfully uninstalled google-cloud-aiplatform-1.79.0\n"]}]},{"cell_type":"code","source":["!pip install langchain-google-genai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3ZGWYag6s9T","executionInfo":{"status":"ok","timestamp":1739812726611,"user_tz":-330,"elapsed":4380,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"bee7d524-7931-40ca-ec10-aab05b20a00a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-google-genai\n","  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n","Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.8.4)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.35)\n","Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.6)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.160.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.26.0)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.3.8)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (24.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.23.0)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.14.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.3.1)\n","Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Installing collected packages: filetype, langchain-google-genai\n","Successfully installed filetype-1.2.0 langchain-google-genai-2.0.9\n"]}]},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAI\n","llm = GoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model = \"gemini-2.0-pro-exp-02-05\", temperature=0)\n","embedding = HuggingFaceEmbeddings()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ju8ANWKVnopw","executionInfo":{"status":"ok","timestamp":1739813265781,"user_tz":-330,"elapsed":1746,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"3a2b6dd2-44ea-4808-9f92-c11bf2cbf501"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-8ad708ef22ba>:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n","  embedding = HuggingFaceEmbeddings()\n"]}]},{"cell_type":"markdown","source":["LangChain Vector store (Backend - AstraDB)"],"metadata":{"id":"8mDanXHbsYSK"}},{"cell_type":"code","source":["astra_vector_store = Cassandra(\n","    embedding = embedding,\n","    table_name = 'pdf_query',\n","    session  = None,\n","    keyspace= None\n",")"],"metadata":{"id":"zht31C6MsU2_","executionInfo":{"status":"ok","timestamp":1739809031449,"user_tz":-330,"elapsed":2836,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","text_splitter = CharacterTextSplitter(\n","    separator = \"\\n\",\n","    chunk_size = 1000,\n","    chunk_overlap = 100,\n","    length_function = len\n","    )\n","texts = text_splitter.split_text(raw_text)"],"metadata":{"id":"RA82OzxPtMRV","executionInfo":{"status":"ok","timestamp":1739809279329,"user_tz":-330,"elapsed":1883,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["texts[:50]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MiabW1HttSF","executionInfo":{"status":"ok","timestamp":1739809319202,"user_tz":-330,"elapsed":408,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"5ca2bbb3-7b72-48f8-d702-8826a5b4f9c4"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['National Engineering College, K.R.Nagar, Kovilpatti -628503  \\n(An Autonomous Institution, Affiliated to Anna University, Chennai)  \\nDepartment of Artificial Intelligence & Data Science  \\nQuestion Bank  \\n                                                                         \\nCourse Code & Title  : 19AD08E & TEXT AND SPEECH ANALYTICS  \\nProgram  : B. Tech (AI & DS)  \\nSemester  : VI \\nRegulation  : 2019  \\nCredit  : 3 \\nCourse Instructors  : Ms. P. Swarna Gowsalya, AP/AI & DS  \\nCourse Coordinator  :   Ms.V.Veera Anusuya, AP/AI & DS  \\n \\nCO1 - Introduction  \\nText Preprocessing and Wrangling – Text tokenization – Stemming – Lemmatization – Removing stop - \\nwords – Feature Engineering for Text representation – Bag of Words model - Bag of N -Grams model – \\nTF-IDF model – Case study: Analysis of Text Preprocessing using NLTK, Implementation of TF -IDF \\nmodels.  \\n \\nPart – A \\n1 State text tokenization in text preprocessing?',\n"," 'models.  \\n \\nPart – A \\n1 State text tokenization in text preprocessing?  \\n\\uf0b7 Tokenization  is the process of dividing text into a set of meaningful pieces. \\nThese pieces are called  tokens .  \\n\\uf0b7 For example, we can divide a chunk of text into words, or we can divide it into \\nsentences.  \\n\\uf0b7 Depending on the task at hand, we can define our own conditions to divide the \\ninput text into meaningful tokens.   \\n\\uf0b7 Example: \"I love NLP\" → [\"I\", \"love\", \"NLP\"].  CO1  \\n2 Define stemming in the context of text preprocessing.  \\n\\uf0b7 Stemming is a text preprocessing technique used in natural language \\nprocessing (NLP) to reduce words to their root or base form.  \\n\\uf0b7 The goal of stemming is to simplify and standardize words, which helps \\nimprove the performance of information retrieval, text classification, and other \\nNLP tasks.  \\n\\uf0b7 By transforming words to their stems, NLP models can treat different forms of \\nthe same word as a single entity, reducing the complexity of the text data',\n"," 'the same word as a single entity, reducing the complexity of the text data  \\n\\uf0b7 Example: \"running\" → \"run\", \"happily\" → \"happili\".  CO1  \\n3 Define is lemmatization and how is it different from stemming?  \\n CO1  \\n4 Explain the role of stop -words in text preprocessing.  CO1  \\n\\uf0b7 Stop-words are common words like \"is,\" \"the,\" and \"and.\"  \\n\\uf0b7 These words carry little meaning and can be removed.  \\n\\uf0b7 Example: \"The cat is sleeping\"  → [\"cat\", \"sleeping\"] . \\n5 List the purpose of removing stop -words during text preprocessing?  \\n\\uf0b7 Reduces Noise in Text Data:  Stop-words like \"is,\" \"the,\" and \"and\" do not \\ncontribute much meaning. Their removal helps in focusing on important \\nwords.  \\n\\uf0b7 Improves Text Analysis Accuracy:  Reducing unnecessary words enhances \\nthe performance of NLP models.  It prevents common words from do minating \\nthe dataset.  \\n\\uf0b7 Enhances Computational Efficiency:  Eliminating stop -words reduces dataset',\n"," 'the dataset.  \\n\\uf0b7 Enhances Computational Efficiency:  Eliminating stop -words reduces dataset \\nsize, speeding up text processing.  It improves efficiency in tasks like sentiment \\nanalysis and classification.  \\n\\uf0b7 Example:  \"She is an excellent teacher\" → [\"excellent\", \"teacher\"].  CO1  \\n6 Define feature engineering in the context of text representation.  \\n\\uf0b7 Feature engineering is the process of transforming raw text data into \\nmeaningful numerical representations for machine learning models.  \\n\\uf0b7 It helps extract important patterns and improve model accuracy in NLP tasks \\nlike text classification, sentiment analysis, and topic modeling.  \\n\\uf0b7 Examples:  \\n1. TF-IDF (Term Frequency -Inverse Document Frequency):  Assigns \\nimportance to words based on their frequency.  \\n2. Word Embeddings (Word2Vec, GloVe):  Converts words into dense \\nvector representations.  \\n3. N-Grams:  Captures word sequences for context understanding.  CO1  \\n7 Define Bag of Words (BoW) model.',\n"," '7 Define Bag of Words (BoW) model.  \\n\\uf0b7 A bag -of-words model is a simple document embedding technique based on \\nword frequency.  \\n\\uf0b7 Through this approach, a model conceptualizes text as a bag of words and \\ntracks the frequency of each word.  \\n\\uf0b7 These frequencies are then converted into numerical values,  which machine \\nlearning algorithms can process and use to extract features from the text.    CO1  \\n8 How does the Bag of NGrams model differ from the BoW model?  \\n    CO1  \\n9 What does TF -IDF stand for and what is its purpose?  \\n\\uf0b7 TF-IDF -Term Frequency -Inverse Document Frequency  \\n\\uf0b7 TF-IDF is a numerical statistic that reflects the significance of a word within a \\ndocument relative to a collection of documents, known as a corpus. The idea \\nbehind TF -IDF is to quantify the importance of a term in a document with \\nrespect to its frequency in the document and its rarity across multiple \\ndocuments.  CO1  \\n10 How is Term Frequency (TF) calculated in the TF -IDF model?',\n"," 'documents.  CO1  \\n10 How is Term Frequency (TF) calculated in the TF -IDF model?  \\n\\uf0b7 TF measures the frequency of a term within a document. It is calculated as the CO1  \\nratio of the number of time s a term occurs in a document to the total number of \\nterms in that document. The goal is to emphasize words that are frequent within \\na document.  \\n                       \\n \\n11 What does Inverse Document Frequency (IDF) measure in the TF -IDF model?  \\n\\uf0b7 IDF measures the rarity of a term across a collection of documents. It is \\ncalculated as the logarithm of the ratio of the total number of documents to the \\nnumber of documents containing the term. The goal is to penalize words that \\nare common across all documen ts. \\n \\n CO1  \\n12 How does tokenization impact the quality of a text dataset?  \\n\\uf0b7 Ensures Proper Text Segmentation : Proper tokenization improves NLP \\ntasks like sentiment analysis and text classification. Incorrect splitting or \\nmissing words can reduce model effectiveness.',\n"," 'missing words can reduce model effectiveness.  \\n\\uf0b7 Affects Model Accuracy and Performance: Proper tokenization improves \\nNLP tasks like sentiment analysis and text classification.  Incorrect splitting or \\nmissing words can reduce model effectiveness.  \\n\\uf0b7 Optimizes Computational Efficiency:  Well -tokenized data reduces \\nprocessing complexity and memory usage.  Helps in efficient feature extraction \\nfor models.  CO1  \\n13 Discriminate  unigrams, bigrams, and trigrams in the context of NGrams?  \\n CO1  \\n14 Why is it important to use lemmatization over stemming in  some NLP tasks?  \\n\\uf0b7 Produces Meaningful Words:  Lemmatization returns actual dictionary \\nwords, while stemming may generate non -existent words.  \\nExample:  \\no Stemming: \"running\"  → \"runn\"  \\no Lemmatization: \"running\"  → \"run\"  \\n\\uf0b7 Improves Context and Accuracy:  Lemmatization considers the word’s \\ncontext and part of speech, making it more suitable for NLP tasks like \\nsentiment analysis and machine translation.',\n"," 'sentiment analysis and machine translation.  \\nExample:  \"better\"  → \"good\"  (lemmatization), while stemming keeps \\n\"better\" . \\n\\uf0b7 Enhances Text Representation in NLP Models:  Models trained on \\nlemmatized text perform better as words retain their correct meanings.  Useful \\nfor information retrieval, where semantically similar words should be grouped.  \\nExample:  \"am\", \"is\", \"are\"  → \"be\"  (lemmatization), but remain unchanged \\nin stemming.  CO1  \\n15 How does the removal of stop -words help improve text analysis?  \\n\\uf0b7 Reduces Noise and Improves Focus  \\n\\uf0b7 Enhances Model Accuracy  \\n\\uf0b7 Speeds Up Processing and Reduces Computational Cost  CO1  \\n16 What are some common techniques used for feature engineering in text analysis?  \\n\\uf0b7 Bag of Words (BoW)  \\n\\uf0b7 TF-IDF (Term Frequency -Inverse Document Frequency)  \\n\\uf0b7 N-Grams (Bigrams, Trigrams)  \\n\\uf0b7 Word Embeddings (Word2Vec, GloVe, FastText)  \\n\\uf0b7 Sentiment Scores and Lexicons  \\n\\uf0b7 Part-of-Speech (POS) Tagging  \\n\\uf0b7 Named Entity Recognition (NER)  CO1',\n"," '\\uf0b7 Part-of-Speech (POS) Tagging  \\n\\uf0b7 Named Entity Recognition (NER)  CO1  \\n17 List the role of NLTK in text preprocessing.  \\n1. Tokenization:  Splits text into words or sentences for further processing.  \\n       Example:  \"I love NLP.\"  → [\"I\", \"love\", \"NLP\", \".\"] . \\n2. Stemming and Lemmatization:  Reduces words to their root or base form to \\nstandardize text.  \\nExample (Stemming):  \"running\" → \"run\"  (using PorterStemmer).  \\nExample (Lemmatization):  \"better\" → \"good\"  (using \\nWordNetLemmatizer).  \\n3. Stop -Word Removal:  Eliminates common words that do not add meaningful \\ninformation.  \\nExample:  \"This is a great movie\"  → [\"great\", \"movie\"] . \\n4. Part -of-Speech (POS) Tagging:  Identifies grammatical roles of words in a \\nsentence.  \\n        Example:  \"Dogs bark\"  → [\"Dogs\" (Noun), \"bark\" (Verb)] . \\n5. Named Entity Recognition (NER):  \\n\\uf0b7 Extracts entities like names, places, and dates.  \\n\\uf0b7 Example:  \"Apple is headquartered in California\"  → [\"Apple\"',\n"," '\\uf0b7 Example:  \"Apple is headquartered in California\"  → [\"Apple\" \\n(Organization), \"California\" (Location)] . \\n6. Text Classification and Sentiment Analysis:  \\n\\uf0b7 Helps categorize text and determine sentiment using b uilt-in corpora and \\nclassifiers  CO1  \\n18 How does the Bag of Words model represent text data?  \\nThe Bag of Words (BoW)  model is a simple and widely used technique for \\nrepresenting text data in natural language processing (NLP) and machine learning. It \\ntransforms text into a numerical format that can be processed by algorithms.  \\nTokenization  \\nVocabulary Creation  \\nVectorizatio n \\nRepresentation  CO1  \\n19 Explain the basic working principle of the TF -IDF model.  CO1  \\n20 Mention the advantage of using TF -IDF over BoW in text representation?  \\n\\uf0b7 Down -weighting Common Words (Stop Words)  \\n\\uf0b7 Highlighting Rare and Informative Words  \\n\\uf0b7 Improved Performance in Text Classification and Retrieval  \\n\\uf0b7 Better Handling of Synonyms and Polysemy',\n"," '\\uf0b7 Better Handling of Synonyms and Polysemy  \\n\\uf0b7 Sensitivity to Corpus Distribution  CO1  \\nPart – B \\n1 Compare and contrast stemming and lemmatization, providing examples of \\neach.  CO1  \\n2 Explain the process of tokenization in text preprocessing and its \\nimportance.  CO1  3 Differentiate Bag of Words and Bag of NGrams models with given example.  \\nSentence 1: \"I love programming.\"  \\nSentence 2: \"Programming is fun.\"  CO1  \\n4 Discuss the steps involved in text preprocessing using NLTK.  CO1  \\n5 How does the TF -IDF model improve upon the Bag of Words model for text \\nrepresentation?  CO1  \\n6 Discuss the key challenges in text preprocessing, and how can they be \\naddressed?  CO1  \\n7 Describe the role and importance of stop-word removal in text analysis with \\nexamples.  CO1  \\n8 Explain the concept of feature engineering in text and how it can be used to \\nenhance text representation.  CO1  \\n9 Explain the working mechanism of the Skip -gram model in Word2Vec with \\nan example.  CO1',\n"," '9 Explain the working mechanism of the Skip -gram model in Word2Vec with \\nan example.  CO1  \\n10 Mention any 2 feature engineering tools and illustrate any one of them.  CO1  \\nPart – C \\n1 Explain in detail the entire process of text preprocessing, from raw text to a \\nstructured format for machine learning models. Include tokenization, \\nstemming, lemmatization, stop -word removal, and feature engineering \\ntechniques.  CO1  \\n2 Apply a bag of N -grams model for the following sentences to find a vector \\nbased on the frequency of each sentence in the N -grams vocabulary.  \\nSentence 1: I love machine learning.  \\nSentence 2: Machine learning is fun.  CO1  \\n3 Discuss the differences between the Bag of Words model and the Bag of \\nNGrams model. How do these models represent text data, and what are \\ntheir advantages and disadvantages?  CO1  \\n4 Explain the difference between Term Frequency (TF), Inverse Document \\nFrequency (IDF), and TF -IDF. Using the following documents, calculate the',\n"," 'Frequency (IDF), and TF -IDF. Using the following documents, calculate the \\nTF-IDF score for the term “data” and “future”:  \\nSentence 1: “Data science is the future of AI”  \\nSentence 2: “Data -driven decisions are crucial for success”  \\nSentence 3: “AI is the future”  CO1  \\n5 Describe the process of analyzing a text corpus using NLTK. Explain the \\nsteps involved in preprocessing, including tokenization, stemming, \\nlemmatization, stop -word remova l, and feature extraction, using NLTK \\nfunctions.  CO1  \\n6 Discuss the advantages and limitations of using TF -IDF over simpler \\nmodels like Bag of Words. Provide examples of when TF -IDF would be \\nmore useful.  CO1  \\n7 In a case study, explain how text preprocessing (tokenization, stemming, \\nlemmatization, stop -word removal) can improve the performance of a \\nmachine learning model for sentiment analysis or text classification tasks.  CO1  \\n \\nCO2 - TEXT CLASSIFICATION',\n"," 'CO2 - TEXT CLASSIFICATION  \\nVector Semantics and Embeddings -Word Embeddings - Word2Vec model – Glove model – Fast \\nText model – Overview of Deep Learning models – RNN – Transformers – Overview of Text \\nsummarization and Topic Models – Case Study.  \\n \\nPart – A \\n1 Define word embeddings  in the context of natural language processing?  \\n\\uf0b7 Word embedding is  a technique used in natural language processing CO2  (NLP) that represents words as numbers so that a computer can work \\nwith them .  \\n\\uf0b7 It is a popular approach for learned numeric representations of  text. \\n\"king\" : [0.4, 0.1, 0.3, 0.7]  \\n\"queen\" : [0.5, 0.2, 0.4, 0.8]  \\n\"man\" : [0.2, 0.1, 0.3, 0.6]  \\n\"woman\" : [0.3, 0.2, 0.3, 0.7]  \\n2 How does the Word2Vec model generate word embeddings?  CO2  \\n3 Differentiate continuous bag of words (CBOW) and Skip -gram in \\nWord2Vec?  CO2  \\n4 Explain the concept of GloVe (Global Vectors for Word Representation) \\nmodel.  CO2  \\n5 How is the FastText model different from Word2Vec?  CO2',\n"," 'model.  CO2  \\n5 How is the FastText model different from Word2Vec?  CO2  \\n6 List the main advantage of using word embeddings over one -hot encoding?  CO2  \\n7 What is the purpose of using deep learning models in NLP tasks?  CO2  \\n8 Briefly explain the architecture of a Recurrent Neural Network (RNN).  CO2  \\n9 How do RNNs handle sequential data?  CO2  \\n10 Define Transformer model and how does it differ from RNNs?  CO2  \\n11 Define attention mechanism in Transformer models?  CO2  \\n12 Define text summarization in NLP.  CO2  \\n13 Point out the two main types of text summarization?  CO2  \\n14 Differentiate extractive and abstractive summarization?  CO2  \\n15 What are topic models in NLP and what is their purpose?  CO2  \\n16 Define Latent Dirichlet Allocation (LDA) as a topic modeling technique.  CO2  \\n17 How do Word2Vec, GloVe, and FastText models differ in generating word \\nembeddings?  CO2  \\n18 How does a Transformer model improve on traditional RNNs for  NLP \\ntasks?  CO2',\n"," '18 How does a Transformer model improve on traditional RNNs for  NLP \\ntasks?  CO2  \\n19 Mention the significance of transfer learning in NLP tasks like text \\nclassification or summarization?  CO2  \\n20 How can text summarization models be evaluated?  CO2  \\nPart – B \\n1 Compare and contrast Word2Vec, GloVe, and FastText  models in terms of how \\nthey generate word embeddings.  CO2  \\n2 Explain the working mechanism of the Skip -gram model in Word2Vec with an \\nexample.  CO2  \\n3 How does the attention mechanism in Transformer models improve performance \\nin NLP tasks?  CO2  \\n4 Describe the role of Recurrent Neural Networks (RNNs) in handling sequential \\ndata.  CO2  \\n5 What are the key differences between extractive and abstractive summarization? \\nProvide examples.  CO2  \\n6 How do deep learning models, specifically RNNs and Transformers, improv e \\ntext summarization tasks?  CO2  \\n7 Discuss the concept of topic modeling and explain how Latent Dirichlet \\nAllocation (LDA) works.  CO2',\n"," 'Allocation (LDA) works.  CO2  \\n8 Explain how FastText handles out -of-vocabulary words and its advantage over \\nWord2Vec and GloVe.  CO2  \\nPart – C \\n1 Tabulate differences between GloVe and Word2Vec in terms of the approach to \\nlearning word embedding’s.  CO2  \\n2 Differences between Skip -gram and CBOW architectures in the Word2Vec CO2  model with appropriate example.  \\n3 Explain in detail about the word2vec m odel.  CO2  \\n4 Explain in detail about the GloVe model.  CO2  \\n5 Structure of a typical deep learning model and the differences between neural \\nnetworks and traditional machine learning algorithms for text processing.  CO2  \\n6 Explain the working mechanism of the FastText model for learning word \\nembeddings. Provide a real -time example of how FastText is used in a practical \\nnatural language processing (NLP) application. Discuss its advantages, \\nlimitations, and how it improves the  quality of the NLP system in the given \\nexample.  CO2',\n"," 'limitations, and how it improves the  quality of the NLP system in the given \\nexample.  CO2  \\n \\nCO3 - QUESTION ANSWERING AND DIALOGUE SYSTEMS  \\nInformation retrieval – IR-based question answering – knowledge -based question answering – \\nlanguage models for QA – classic QA models – chatbots  – Design of dialogue systems -\\nevaluating dialogue systems Case Study : Developing a knowledge -based question -answering \\nsystem, Classic QA model development.  \\n \\nPart – A \\n1 Define information retrieval (IR) in the context of natural language processing.  CO3  \\n2 Define knowledge -based question answering system.  CO3  \\n3 How do language models contribute to question answering (QA) systems?  CO3  \\n4 What are classic QA models in information retrieval?  CO3  \\n5 Define chatbot, and how does it function in dialogue systems.  CO3  \\n6 List out the key components of a dialogue system.  CO3  \\n7 Define a language model for question answering.  CO3',\n"," '7 Define a language model for question answering.  CO3  \\n8 Differentiate information retrieval -based QA and knowledge -based QA.  CO3  \\n9 What is the purpose of designing a dialogue system in the context of natural \\nlanguage understanding?  CO3  \\n10 How does a knowledge -based question answering system work?  CO3  \\n11 Mention the significance of evaluating dialogue systems?  CO3  \\n12 How does a classic QA model retrieve information from a database or corpus?  CO3  \\n13 Checklist the main challenges in developing an information retrieval -based \\nquestion answering system.  CO3  \\n14 List the typical stages in the design of a dialogue system.  CO3  \\n15 Mention the role of natural language processing (NLP) play in chatbots and \\ndialogue systems?  CO3  \\n16 Difference between rule -based and machine learning -based chatbots.  CO3  \\n17 Define the term \"language model\" in the context of QA systems.  CO3  \\n18 Point out some common evaluation metrics for dialogue systems.  CO3',\n"," '18 Point out some common evaluation metrics for dialogue systems.  CO3  \\n19 How do knowledge -based question answering systems handle ambiguous \\nquestions?  CO3  \\n20 Mention the role of context in the development of dialogue systems.  CO3  \\nPart – B \\n1 Compare and contrast information retrieval -based question answering systems \\nwith knowledge -based question answering systems.  CO3  \\n2 Explain the key components involved in designing a dialogue system.  CO3  \\n3 Describe the process of developing a knowledge -based question answering \\nsystem, focusing on its components and workflow.  CO3  \\n4 How do classic QA models work in information retrieval? Provide an example \\nof a classic QA system.  CO3  \\n5 Discuss the importance of evaluating dialogue systems. What are the common CO3  evaluation methods?  \\n6 What are the key differences between a chatbot a nd a dialogue system?  CO3  \\n7 Explain how language models, such as BERT or GPT, are used for question \\nanswering tasks.  CO3',\n"," '7 Explain how language models, such as BERT or GPT, are used for question \\nanswering tasks.  CO3  \\n8 Describe the process of designing and implementing a simple chatbot for a \\nspecific domain.  CO3  \\nPart – C \\n1 Explain the working principles of information retrieval -based and knowledge -\\nbased question answering systems. Discuss their advantages, limitations, and \\ntypical use cases.  CO3  \\n2 Provide a detailed overview of the process of designing and developing a \\ndialogue system. Discuss  the components involved, including natural language \\nunderstanding, dialogue management, and natural language generation.  CO3  \\n3 Discuss the role of language models (like GPT, BERT, or T5) in modern \\nquestion answering systems. Provide examples of how these  models are applied \\nto answer questions in various domains.  CO3  \\n4 Explain the concept of classic QA models in information retrieval. Provide an \\nexample of a QA system based on information retrieval, detailing its architecture',\n"," 'example of a QA system based on information retrieval, detailing its architecture \\nand working mechanism.  CO3  \\n5 Discuss the challenges involved in developing knowledge -based question \\nanswering systems. How do these systems handle issues like ambiguity, context, \\nand domain -specific knowledge?  CO3  \\n6 Develop a case study for a knowledge -based question answering syste m. \\nDescribe the design, components, and key challenges faced during development, \\nalong with the system’s performance evaluation.  CO3']"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Load the Data into the Vector Store"],"metadata":{"id":"jMAJsXr9vHH8"}},{"cell_type":"code","source":["astra_vector_store.add_texts(texts[:50])\n","print(\"%i number of Data loaded into the Vector Store\" % len(texts[:50]))\n","\n","astra_vector_index = VectorStoreIndexWrapper(vectorstore = astra_vector_store)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnq-hpEdvFPK","executionInfo":{"status":"ok","timestamp":1739809821205,"user_tz":-330,"elapsed":26276,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"357fdc4b-ed85-45fa-fb2a-07a0f67237b4"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["22 number of Data loaded into the Vector Store\n"]}]},{"cell_type":"code","source":["\n","first_question = True\n","while True:\n","  if first_question:\n","    question = input(\"Enter your question or type 'quit' to exit : \").strip()\n","  else:\n","    question = input(\"\\nDo you have any other queries (or type 'quit' to exit): \").strip()\n","\n","  if question.lower() == \"quit\":\n","    break\n","  if question == \"\":\n","    continue\n","\n","  first_question = False\n","\n","  response = astra_vector_index.query(question, llm = llm)\n","  print(response)\n","\n","  for answer, score in astra_vector_store.similarity_search_with_score(question, k = 4):\n","    print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, answer.page_content[:84]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXBlD7y9xrvQ","executionInfo":{"status":"ok","timestamp":1739813918843,"user_tz":-330,"elapsed":55830,"user":{"displayName":"Mahrooz","userId":"07071138624173181777"}},"outputId":"da28f2d7-6f56-4a31-8e11-4f6ae7c2b713"},"execution_count":43,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your question or type 'quit' to exit : TF-IDF\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n","WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"]},{"name":"stdout","output_type":"stream","text":["Here's a breakdown of how to calculate the TF-IDF scores for the terms \"data\" and \"future\" based on the provided context:\n","\n","**Understanding TF-IDF**\n","\n","*   **TF (Term Frequency):** Measures how often a term appears in a document.\n","    *   TF = (Number of times the term appears in the document) / (Total number of terms in the document)\n","\n","*   **IDF (Inverse Document Frequency):** Measures how rare a term is across all documents.\n","    *   IDF = log(Total number of documents / Number of documents containing the term)\n","\n","*   **TF-IDF:** The product of TF and IDF. It gives a high score to terms that are frequent in a specific document but rare across the entire corpus.\n","    *   TF-IDF = TF \\* IDF\n","\n","**Calculations**\n","**1. Term Frequency (TF):**\n","*   **\"Data\":**\n","    *   Sentence 1: 1/7\n","    *   Sentence 2: 1/7\n","    *   Sentence 3: 0/4\n","*   **\"Future\":**\n","    *   Sentence 1: 1/7\n","    *   Sentence 2: 0/7\n","    *   Sentence 3: 1/4\n","\n","**2. Inverse Document Frequency (IDF):**\n","*    Total number of documents:3\n","*   **\"Data\":**\n","    *   Documents containing \"data\": 2\n","    *   IDF = log(3/2)\n","*   **\"Future\":**\n","    *   Documents containing \"future\": 2\n","    *   IDF = log(3/2)\n","\n","**3. TF-IDF:**\n","*  **\"Data\":**\n","    * Sentence 1: 1/7 * log(3/2)\n","    * Sentence 2: 1/7 * log(3/2)\n","    * Sentence 3: 0\n","* **\"Future\":**\n","    * Sentence 1: 1/7 * log(3/2)\n","    * Sentence 2: 0\n","    * Sentence 3: 1/4 * log(3/2)\n","\n","    [0.8288] \"Frequency (IDF), and TF -IDF. Using the following documents, calculate the \n","TF-IDF s ...\"\n","    [0.8288] \"Frequency (IDF), and TF -IDF. Using the following documents, calculate the \n","TF-IDF s ...\"\n","    [0.8268] \"documents.  CO1  \n","10 How is Term Frequency (TF) calculated in the TF -IDF model?  \n"," ...\"\n","    [0.8268] \"documents.  CO1  \n","10 How is Term Frequency (TF) calculated in the TF -IDF model?  \n"," ...\"\n","\n","Do you have any other queries (or type 'quit' to exit): semantic\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n","WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"]},{"name":"stdout","output_type":"stream","text":["I don't have enough information to answer the question. The provided context defines feature engineering, stop-word removal, Bag of Words, and discusses question-answering systems, but it doesn't define the term \"semantic\".\n","\n","    [0.6599] \"example of a QA system based on information retrieval, detailing its architecture \n","a ...\"\n","    [0.6599] \"example of a QA system based on information retrieval, detailing its architecture \n","a ...\"\n","    [0.6571] \"the dataset.  \n"," Enhances Computational Efficiency:  Eliminating stop -words reduces ...\"\n","    [0.6571] \"the dataset.  \n"," Enhances Computational Efficiency:  Eliminating stop -words reduces ...\"\n","\n","Do you have any other queries (or type 'quit' to exit): quit\n"]}]}]}